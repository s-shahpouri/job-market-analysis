{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SCIENCE JOB MARKET ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Scraping using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #create an instance of browser\n",
    "# driver = webdriver.Chrome()\n",
    "\n",
    "# #creating a dictionary for storing the information after scraping\n",
    "# jobs={\"roles\":[],\n",
    "#      \"companies\":[],\n",
    "#      \"locations\":[],\n",
    "#      \"experience\":[],\n",
    "#      \"skills\":[]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSS_SELECTOR found 25 job posts\n",
      "CSS_SELECTOR found 25 job posts\n",
      "CSS_SELECTOR found 20 job posts\n",
      "CSS_SELECTOR found 20 job posts\n",
      "CSS_SELECTOR found 20 job posts\n"
     ]
    }
   ],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# job_cards = []\n",
    "# for i in range(5):\n",
    "#     driver.get(\"https://www.naukri.com/data-scientist-jobs-{}\".format(i))\n",
    "    \n",
    "#     # منتظر لود کامل شغل‌ها\n",
    "#     WebDriverWait(driver, 10).until(\n",
    "#         EC.presence_of_element_located((By.CSS_SELECTOR, \"div.cust-job-tuple\"))\n",
    "#     )\n",
    "\n",
    "#     jobs_cards = driver.find_elements(By.CSS_SELECTOR, \"div.cust-job-tuple\")\n",
    "#     print(f\"CSS_SELECTOR found {len(jobs_cards)} job posts\")\n",
    "    \n",
    "#     for job in job_cards:\n",
    "#         html = job.get_attribute(\"outerHTML\")\n",
    "#         soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#         try:\n",
    "#             role = soup.select_one(\"a.title\").text.strip()\n",
    "#         except:\n",
    "#             role = \"\"\n",
    "#         try:\n",
    "#             company = soup.select_one(\"a.comp-name\").text.strip()\n",
    "#         except:\n",
    "#             company = \"\"\n",
    "#         try:\n",
    "#             exp = soup.select_one(\"span.expwdth\").text.strip()\n",
    "#         except:\n",
    "#             exp = \"\"\n",
    "#         try:\n",
    "#             location = soup.select_one(\"span.locWdth\").text.strip()\n",
    "#         except:\n",
    "#             location = \"\"\n",
    "#         try:\n",
    "#             skills = \", \".join([li.text.strip() for li in soup.select(\"ul.tags-gt li\")])\n",
    "#         except:\n",
    "#             skills = \"\"\n",
    "\n",
    "#         jobs[\"roles\"].append(role)\n",
    "#         jobs[\"companies\"].append(company)\n",
    "#         jobs[\"locations\"].append(location)\n",
    "#         jobs[\"experience\"].append(exp)\n",
    "#         jobs[\"skills\"].append(skills)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# DS_jobs_df=pd.DataFrame(jobs)\n",
    "# DS_jobs_df.to_csv(\"DataScience_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# import time\n",
    "\n",
    "# driver = webdriver.Chrome()\n",
    "\n",
    "# for i in range(1):\n",
    "#     driver.get(\"https://www.naukri.com/data-scientist-jobs-{}\".format(i))\n",
    "    \n",
    "#     # منتظر لود کامل شغل‌ها\n",
    "#     WebDriverWait(driver, 10).until(\n",
    "#         EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'cust-job-tuple')]\"))\n",
    "#     )\n",
    "\n",
    "#     jobs_xpath = driver.find_elements(By.XPATH, \"//div[contains(@class, 'cust-job-tuple')]\")\n",
    "#     print(f\"XPath found {len(jobs_xpath)} job posts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting details of each job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for job in job_cards:\n",
    "#     html = job.get_attribute(\"outerHTML\")\n",
    "#     soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#     try:\n",
    "#         role = soup.select_one(\"a.title\").text.strip()\n",
    "#     except:\n",
    "#         role = \"\"\n",
    "#     try:\n",
    "#         company = soup.select_one(\"a.comp-name\").text.strip()\n",
    "#     except:\n",
    "#         company = \"\"\n",
    "#     try:\n",
    "#         exp = soup.select_one(\"span.expwdth\").text.strip()\n",
    "#     except:\n",
    "#         exp = \"\"\n",
    "#     try:\n",
    "#         location = soup.select_one(\"span.locWdth\").text.strip()\n",
    "#     except:\n",
    "#         location = \"\"\n",
    "#     try:\n",
    "#         skills = \", \".join([li.text.strip() for li in soup.select(\"ul.tags-gt li\")])\n",
    "#     except:\n",
    "#         skills = \"\"\n",
    "\n",
    "#     jobs[\"roles\"].append(role)\n",
    "#     jobs[\"companies\"].append(company)\n",
    "#     jobs[\"locations\"].append(location)\n",
    "#     jobs[\"experience\"].append(exp)\n",
    "#     jobs[\"skills\"].append(skills)\n",
    "\n",
    "# import pandas as pd\n",
    "# DS_jobs_df=pd.DataFrame(jobs)\n",
    "# DS_jobs_df.to_csv(\"DataScience_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping in Python: Selenium vs BeautifulSoup\n",
    "\n",
    "## Introduction\n",
    "In data science, scraping web data is a crucial skill. Two popular tools for this are **Selenium** and **BeautifulSoup (bs4)**. Each tool is best suited for different situations. This guide helps you understand when and why to use each.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Selenium\n",
    "\n",
    "Selenium is ideal when you need to interact with a website like a human would:\n",
    "\n",
    "- The website loads content dynamically using JavaScript.\n",
    "- You need to click buttons, log in, scroll, or wait for content to appear.\n",
    "- **Example:** Scraping Naukri.com job listings where content is loaded after the page renders.\n",
    "\n",
    "**Advantages:**\n",
    "- Can handle dynamic JS-rendered content.\n",
    "- Allows interaction like a browser (click, input, etc).\n",
    "\n",
    "**Disadvantages:**\n",
    "- Slower, as it runs a full browser.\n",
    "- Requires installing browser drivers like ChromeDriver.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use BeautifulSoup\n",
    "\n",
    "BeautifulSoup is best when you already have access to raw HTML content:\n",
    "\n",
    "- The page's data is fully loaded in the static HTML.\n",
    "- You are reading saved HTML files.\n",
    "\n",
    "**Advantages:**\n",
    "- Extremely fast and lightweight.\n",
    "- Very easy to use and requires no browser setup.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Cannot execute JavaScript.\n",
    "- Cannot interact with websites (no clicks, scrolls, etc).\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Comparison from This Project\n",
    "\n",
    "In our Naukri.com scraping example:\n",
    "\n",
    "- We used Selenium to load job cards and wait for JavaScript to render the page.\n",
    "- Initially, Selenium couldn’t extract some elements using `find_element`, even though they existed in `outerHTML`.\n",
    "- To solve this, we parsed each job's HTML using BeautifulSoup and extracted data reliably.\n",
    "\n",
    "**Conclusion:**\n",
    "- Use **Selenium** to access complex, interactive, JS-based content.\n",
    "- Use **BeautifulSoup** to parse raw HTML when content is already present or extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "jobs = {\n",
    "    \"roles\": [],\n",
    "    \"companies\": [],\n",
    "    \"locations\": [],\n",
    "    \"experience\": [],\n",
    "    \"skills\": []\n",
    "}\n",
    "\n",
    "for i in range(50):\n",
    "    driver.get(f\"https://www.naukri.com/data-scientist-jobs-{i}\")\n",
    "    \n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.cust-job-tuple\"))\n",
    "    )\n",
    "    \n",
    "    job_cards = driver.find_elements(By.CSS_SELECTOR, \"div.cust-job-tuple\")\n",
    "\n",
    "    for job in job_cards:\n",
    "        try:\n",
    "            role = job.find_element(By.CSS_SELECTOR, \"a.title\").text.strip()\n",
    "        except:\n",
    "            role = \"\"\n",
    "        try:\n",
    "            company = job.find_element(By.CSS_SELECTOR, \"a.comp-name\").text.strip()\n",
    "        except:\n",
    "            company = \"\"\n",
    "        try:\n",
    "            exp = job.find_element(By.CSS_SELECTOR, \"span.exp span.expwdth\").text.strip()\n",
    "        except:\n",
    "            exp = \"\"\n",
    "        try:\n",
    "            location = job.find_element(By.CSS_SELECTOR, \"span.loc span.locWdth\").text.strip()\n",
    "        except:\n",
    "            location = \"\"\n",
    "        try:\n",
    "            skills_list = job.find_elements(By.CSS_SELECTOR, \"ul.tags-gt li\")\n",
    "            skills = ', '.join([s.text.strip() for s in skills_list])\n",
    "        except:\n",
    "            skills = \"\"\n",
    "\n",
    "        jobs[\"roles\"].append(role)\n",
    "        jobs[\"companies\"].append(company)\n",
    "        jobs[\"locations\"].append(location)\n",
    "        jobs[\"experience\"].append(exp)\n",
    "        jobs[\"skills\"].append(skills)\n",
    "\n",
    "# # ذخیره در CSV\n",
    "# with open(\"naukri_jobs2.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(jobs.keys())\n",
    "#     writer.writerows(zip(*jobs.values()))\n",
    "\n",
    "# print(\"✅ File saved as 'naukri_jobs2.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DS_jobs_df=pd.DataFrame(jobs)\n",
    "DS_jobs_df.to_csv(\"DataScience_jobs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobenv",
   "language": "python",
   "name": "jobenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
